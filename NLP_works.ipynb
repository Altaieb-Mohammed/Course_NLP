{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1UIZFRtXRBTNmQPpDNVi4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Altaieb-Mohammed/Course_NLP/blob/main/NLP_works.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "52ktAPLmIsRn",
        "outputId": "b7d4eb52-4ce5-4e28-9daf-c949a3e5089d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "CSV file not found: train.csv",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-04ab6eb53d5b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mtext_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'english_text'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mtrain_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_from_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_csv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mval_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_from_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_csv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-04ab6eb53d5b>\u001b[0m in \u001b[0;36mload_data_from_csv\u001b[0;34m(file_path, text_col, label_col)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data_from_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english_text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"CSV file not found: {file_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtext_col\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlabel_col\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: CSV file not found: train.csv"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "def load_data_from_csv(file_path, text_col='english_text', label_col='label'):\n",
        "    if not os.path.isfile(file_path):\n",
        "        raise FileNotFoundError(f\"CSV file not found: {file_path}\")\n",
        "    df = pd.read_csv(file_path)\n",
        "    if text_col not in df.columns or label_col not in df.columns:\n",
        "        raise ValueError(f\"CSV must contain columns '{text_col}' and '{label_col}'\")\n",
        "    texts = df[text_col].astype(str).tolist()\n",
        "    labels = df[label_col].tolist()\n",
        "    return texts, labels\n",
        "\n",
        "def tfidf_svm_train_predict(train_texts, train_labels, val_texts, val_labels):\n",
        "    print(\"Начинаем обучение TF-IDF + SVM...\")\n",
        "    pipeline = make_pipeline(TfidfVectorizer(), LinearSVC())\n",
        "    pipeline.fit(train_texts, train_labels)\n",
        "    val_preds = pipeline.predict(val_texts)\n",
        "    print(\"TF-IDF + SVM Validation Accuracy:\", accuracy_score(val_labels, val_preds))\n",
        "    print(classification_report(val_labels, val_preds, zero_division=0))\n",
        "    print(\"TF-IDF + SVM завершено.\\n\")\n",
        "    return val_preds\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_csv_path = \"train.csv\"\n",
        "    val_csv_path = \"validation.csv\"\n",
        "    text_column = 'english_text'\n",
        "\n",
        "    train_texts, train_labels = load_data_from_csv(train_csv_path, text_col=text_column)\n",
        "    val_texts, val_labels = load_data_from_csv(val_csv_path, text_col=text_column)\n",
        "\n",
        "    tfidf_svm_train_predict(train_texts, train_labels, val_texts, val_labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "import argparse\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "from typing import List, Tuple, Union\n",
        "\n",
        "try:\n",
        "    from gensim.models import KeyedVectors\n",
        "except ImportError:\n",
        "    KeyedVectors = None\n",
        "\n",
        "try:\n",
        "    import fasttext\n",
        "except ImportError:\n",
        "    fasttext = None\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "def set_seed(seed: int = 42) -> None:\n",
        "    \"\"\"Set random seed for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    logging.info(f\"Random seed set to {seed}\")\n",
        "\n",
        "\n",
        "def load_data_from_csv(file_path: str, text_col: str = 'english_text', label_col: str = 'label') -> Tuple[List[str], List[int]]:\n",
        "    \"\"\"\n",
        "    Load texts and labels from a CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to the CSV file.\n",
        "        text_col: Column name for text data.\n",
        "        label_col: Column name for labels.\n",
        "\n",
        "    Returns:\n",
        "        texts: List of text strings.\n",
        "        labels: List of labels.\n",
        "    \"\"\"\n",
        "    if not os.path.isfile(file_path):\n",
        "        raise FileNotFoundError(f\"CSV file not found: {file_path}\")\n",
        "    df = pd.read_csv(file_path)\n",
        "    if text_col not in df.columns or label_col not in df.columns:\n",
        "        raise ValueError(f\"CSV must contain columns '{text_col}' and '{label_col}'\")\n",
        "    texts = df[text_col].astype(str).tolist()\n",
        "    labels = df[label_col].tolist()\n",
        "    logging.info(f\"Loaded {len(texts)} samples from {file_path}\")\n",
        "    return texts, labels\n",
        "\n",
        "\n",
        "def load_fasttext_embeddings(path: str, embedding_dim: int = 300) -> Union[KeyedVectors, fasttext.FastText._FastText]:\n",
        "    \"\"\"\n",
        "    Load FastText embeddings from a .vec or .bin file.\n",
        "\n",
        "    Args:\n",
        "        path: Path to the embeddings file.\n",
        "        embedding_dim: Dimension of embeddings.\n",
        "\n",
        "    Returns:\n",
        "        Loaded FastText model.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Loading FastText embeddings from {path} ...\")\n",
        "    if path.endswith('.bin'):\n",
        "        if fasttext is None:\n",
        "            raise ImportError(\"fasttext library is not installed. Install it with `pip install fasttext` to load .bin files.\")\n",
        "        model = fasttext.load_model(path)\n",
        "        logging.info(\"FastText binary model loaded.\")\n",
        "        return model\n",
        "    else:\n",
        "        if KeyedVectors is None:\n",
        "            raise ImportError(\"gensim is not installed. Install it with `pip install gensim` to load .vec files.\")\n",
        "        model = KeyedVectors.load_word2vec_format(path, binary=False)\n",
        "        logging.info(\"FastText text vector model loaded.\")\n",
        "        return model\n",
        "\n",
        "\n",
        "def fasttext_vectorize(text: str, ft_model: Union[KeyedVectors, fasttext.FastText._FastText], embedding_dim: int = 300) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Vectorize a single text using FastText embeddings.\n",
        "\n",
        "    Args:\n",
        "        text: Input text string.\n",
        "        ft_model: Loaded FastText model.\n",
        "        embedding_dim: Embedding dimension.\n",
        "\n",
        "    Returns:\n",
        "        Mean vector of word embeddings.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    vectors = []\n",
        "    if hasattr(ft_model, 'get_word_vector'):\n",
        "        # fasttext binary model\n",
        "        vectors = [ft_model.get_word_vector(word) for word in words]\n",
        "    else:\n",
        "        # gensim KeyedVectors\n",
        "        vectors = [ft_model[word] for word in words if word in ft_model]\n",
        "\n",
        "    if not vectors:\n",
        "        return np.zeros(embedding_dim, dtype=np.float32)\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "\n",
        "def batch_vectorize(texts: List[str], ft_model: Union[KeyedVectors, fasttext.FastText._FastText], embedding_dim: int = 300, batch_size: int = 512) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Vectorize a list of texts in batches.\n",
        "\n",
        "    Args:\n",
        "        texts: List of text strings.\n",
        "        ft_model: Loaded FastText model.\n",
        "        embedding_dim: Embedding dimension.\n",
        "        batch_size: Number of texts to process per batch.\n",
        "\n",
        "    Returns:\n",
        "        Array of vectorized texts.\n",
        "    \"\"\"\n",
        "    vectors = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Vectorizing texts\"):\n",
        "        batch = texts[i:i + batch_size]\n",
        "        batch_vecs = [fasttext_vectorize(text, ft_model, embedding_dim) for text in batch]\n",
        "        vectors.extend(batch_vecs)\n",
        "    return np.array(vectors, dtype=np.float32)\n",
        "\n",
        "\n",
        "def fasttext_train_predict(train_texts: List[str], train_labels: List[int], val_texts: List[str], val_labels: List[int],\n",
        "                           ft_model: Union[KeyedVectors, fasttext.FastText._FastText], embedding_dim: int = 300) -> List[int]:\n",
        "    \"\"\"\n",
        "    Train Logistic Regression on FastText embeddings and predict on validation data.\n",
        "\n",
        "    Args:\n",
        "        train_texts: Training texts.\n",
        "        train_labels: Training labels.\n",
        "        val_texts: Validation texts.\n",
        "        val_labels: Validation labels.\n",
        "        ft_model: Loaded FastText model.\n",
        "        embedding_dim: Embedding dimension.\n",
        "\n",
        "    Returns:\n",
        "        Validation predictions.\n",
        "    \"\"\"\n",
        "    logging.info(\"Starting training FastText + Logistic Regression...\")\n",
        "    X_train = batch_vectorize(train_texts, ft_model, embedding_dim)\n",
        "    X_val = batch_vectorize(val_texts, ft_model, embedding_dim)\n",
        "\n",
        "    clf = LogisticRegression(max_iter=5000, n_jobs=-1)\n",
        "    clf.fit(X_train, train_labels)\n",
        "\n",
        "    val_preds = clf.predict(X_val)\n",
        "\n",
        "    acc = accuracy_score(val_labels, val_preds)\n",
        "    logging.info(f\"Validation Accuracy: {acc:.4f}\")\n",
        "    logging.info(\"Classification Report:\\n\" + classification_report(val_labels, val_preds, zero_division=0))\n",
        "    logging.info(\"Training completed.\\n\")\n",
        "\n",
        "    return val_preds.tolist()\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"FastText + Logistic Regression Text Classification\")\n",
        "    parser.add_argument(\"--train_csv\", type=str, default=\"train.csv\", help=\"Path to training CSV file\")\n",
        "    parser.add_argument(\"--val_csv\", type=str, default=\"validation.csv\", help=\"Path to validation CSV file\")\n",
        "    parser.add_argument(\"--text_col\", type=str, default=\"english_text\", help=\"Name of the text column\")\n",
        "    parser.add_argument(\"--label_col\", type=str, default=\"label\", help=\"Name of the label column\")\n",
        "    parser.add_argument(\"--fasttext_path\", type=str, default=\"cc.ru.300.vec\", help=\"Path to FastText embeddings file (.vec or .bin)\")\n",
        "    parser.add_argument(\"--embedding_dim\", type=int, default=300, help=\"Dimension of FastText embeddings\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')\n",
        "\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    try:\n",
        "        train_texts, train_labels = load_data_from_csv(args.train_csv, args.text_col, args.label_col)\n",
        "        val_texts, val_labels = load_data_from_csv(args.val_csv, args.text_col, args.label_col)\n",
        "    except (FileNotFoundError, ValueError) as e:\n",
        "        logging.error(e)\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(args.fasttext_path):\n",
        "        logging.error(f\"FastText embeddings file not found at {args.fasttext_path}. Skipping FastText model.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        ft_model = load_fasttext_embeddings(args.fasttext_path, args.embedding_dim)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to load FastText embeddings: {e}\")\n",
        "        return\n",
        "\n",
        "    fasttext_train_predict(train_texts, train_labels, val_texts, val_labels, ft_model, args.embedding_dim)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "6xwGTjiCI-i6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "def load_data_from_csv(file_path, text_col='english_text', label_col='label'):\n",
        "    if not os.path.isfile(file_path):\n",
        "        raise FileNotFoundError(f\"CSV file not found: {file_path}\")\n",
        "    df = pd.read_csv(file_path)\n",
        "    if text_col not in df.columns or label_col not in df.columns:\n",
        "        raise ValueError(f\"CSV must contain columns '{text_col}' and '{label_col}'\")\n",
        "    texts = df[text_col].astype(str).tolist()\n",
        "    labels = df[label_col].tolist()\n",
        "    return texts, labels\n",
        "\n",
        "def load_glove_embeddings(path, word_to_idx, embedding_dim=100):\n",
        "    print(f\"Loading GloVe embeddings from {path} ...\")\n",
        "    embedding_matrix = np.zeros((len(word_to_idx), embedding_dim))\n",
        "    found = 0\n",
        "    with open(path, encoding=\"utf8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            word = parts[0]\n",
        "            if word in word_to_idx:\n",
        "                embedding_matrix[word_to_idx[word]] = np.array(parts[1:], dtype=np.float32)\n",
        "                found += 1\n",
        "    print(f\"Found embeddings for {found} words out of {len(word_to_idx)}\")\n",
        "    return torch.tensor(embedding_matrix, dtype=torch.float32)\n",
        "\n",
        "def build_vocab(texts):\n",
        "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
        "    for text in texts:\n",
        "        for word in text.split():\n",
        "            if word not in vocab:\n",
        "                vocab[word] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "def text_to_indices(texts, vocab, max_len=100):\n",
        "    indices = []\n",
        "    for text in texts:\n",
        "        idxs = [vocab.get(word, vocab['<UNK>']) for word in text.split()]\n",
        "        idxs = (idxs + [vocab['<PAD>']] * max_len)[:max_len]\n",
        "        indices.append(idxs)\n",
        "    return torch.tensor(indices, dtype=torch.long)\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_dim, output_dim, dropout=0.3):\n",
        "        super().__init__()\n",
        "        num_embeddings, embedding_dim = embedding_matrix.shape\n",
        "        self.embedding = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n",
        "        self.embedding.weight = nn.Parameter(embedding_matrix)\n",
        "        self.embedding.weight.requires_grad = False\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        _, (hidden, _) = self.lstm(embedded)\n",
        "        hidden_cat = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
        "        return self.fc(self.dropout(hidden_cat))\n",
        "\n",
        "def train_bilstm(train_texts, train_labels, val_texts, val_labels, glove_path, num_classes,\n",
        "                 epochs=10, batch_size=32, max_len=100, hidden_dim=128, lr=0.001, device=None):\n",
        "    print(\"Начинаем обучение BiLSTM...\")\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    vocab = build_vocab(train_texts + val_texts)\n",
        "    embedding_matrix = load_glove_embeddings(glove_path, vocab, embedding_dim=100)\n",
        "    X_train = text_to_indices(train_texts, vocab, max_len)\n",
        "    X_val = text_to_indices(val_texts, vocab, max_len)\n",
        "    y_train = torch.tensor(train_labels, dtype=torch.long)\n",
        "    y_val = torch.tensor(val_labels, dtype=torch.long)\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    val_dataset = TensorDataset(X_val, y_val)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "    model = BiLSTM(embedding_matrix, hidden_dim, num_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    best_val_acc = 0\n",
        "    patience = 3\n",
        "    patience_counter = 0\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_correct = 0\n",
        "        total = 0\n",
        "        for x_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} training\"):\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(x_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * x_batch.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            train_correct += (preds == y_batch).sum().item()\n",
        "            total += x_batch.size(0)\n",
        "        train_acc = train_correct / total\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in val_loader:\n",
        "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "                outputs = model(x_batch)\n",
        "                preds = outputs.argmax(dim=1)\n",
        "                val_correct += (preds == y_batch).sum().item()\n",
        "                val_total += x_batch.size(0)\n",
        "        val_acc = val_correct / val_total\n",
        "        print(f\"Epoch {epoch+1}: Train Loss={train_loss/total:.4f}, Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}\")\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), \"best_bilstm.pt\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "    model.load_state_dict(torch.load(\"best_bilstm.pt\"))\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    with torch.no_grad():\n",
        "        for x_batch, _ in val_loader:\n",
        "            x_batch = x_batch.to(device)\n",
        "            outputs = model(x_batch)\n",
        "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "    print(\"BiLSTM Validation Accuracy:\", accuracy_score(val_labels, all_preds))\n",
        "    print(classification_report(val_labels, all_preds, zero_division=0))\n",
        "    print(\"BiLSTM обучение завершено.\\n\")\n",
        "    return all_preds\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_csv_path = \"train.csv\"\n",
        "    val_csv_path = \"validation.csv\"\n",
        "    text_column = 'english_text'\n",
        "    glove_path = \"glove.6B.100d.txt\"\n",
        "\n",
        "    train_texts, train_labels = load_data_from_csv(train_csv_path, text_col=text_column)\n",
        "    val_texts, val_labels = load_data_from_csv(val_csv_path, text_col=text_column)\n",
        "    num_classes = len(set(train_labels))\n",
        "\n",
        "    if os.path.exists(glove_path):\n",
        "        train_bilstm(train_texts, train_labels, val_texts, val_labels, glove_path, num_classes)\n",
        "    else:\n",
        "        print(f\"GloVe embeddings file not found at {glove_path}. Skipping BiLSTM model.\")\n"
      ],
      "metadata": {
        "id": "gIFdsWt0I_Gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from datasets import Dataset\n",
        "from tqdm import tqdm\n",
        "from gensim.models import KeyedVectors\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Load dataset from CSV\n",
        "def load_data_from_csv(file_path, text_col='english_text', label_col='label'):\n",
        "    if not os.path.isfile(file_path):\n",
        "        raise FileNotFoundError(f\"CSV file not found: {file_path}\")\n",
        "    df = pd.read_csv(file_path)\n",
        "    if text_col not in df.columns or label_col not in df.columns:\n",
        "        raise ValueError(f\"CSV must contain columns '{text_col}' and '{label_col}'\")\n",
        "    texts = df[text_col].astype(str).tolist()\n",
        "    labels = df[label_col].tolist()\n",
        "    return texts, labels\n",
        "\n",
        "# 4. Fine-tuning mBERT\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc}\n",
        "\n",
        "def mbert_train(train_texts, train_labels, val_texts, val_labels, num_classes,\n",
        "                model_name=\"bert-base-multilingual-cased\", batch_size=8, epochs=3, lr=2e-5, max_len=128):\n",
        "    print(\"Начинаем обучение mBERT...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=max_len)\n",
        "    train_dataset = Dataset.from_dict({\"text\": train_texts, \"label\": train_labels})\n",
        "    val_dataset = Dataset.from_dict({\"text\": val_texts, \"label\": val_labels})\n",
        "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "    val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "    train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "    val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./mbert_output\",\n",
        "        eval_strategy=\"epoch\",          # Correct argument name\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=lr,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=epochs,\n",
        "        weight_decay=0.01,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        save_total_limit=2,\n",
        "        seed=42,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=50,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        "    )\n",
        "    trainer.train()\n",
        "    print(\"mBERT обучение завершено.\\n\")\n",
        "    return model, tokenizer\n",
        "\n",
        "def mbert_predict(model, tokenizer, texts, labels, batch_size=8, max_len=128, device=None):\n",
        "    print(\"Начинаем предсказание mBERT...\")\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Predicting\"):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        encodings = tokenizer(batch_texts, truncation=True, padding=True, max_length=max_len, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encodings)\n",
        "            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "    print(\"Fine-tuned mBERT Validation Accuracy:\", accuracy_score(labels, all_preds))\n",
        "    print(classification_report(labels, all_preds, zero_division=0))\n",
        "    print(\"Предсказание mBERT завершено.\\n\")\n",
        "    return all_preds\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    train_csv_path = \"train.csv\"       # Your training CSV path\n",
        "    val_csv_path = \"validation.csv\"    # Your validation CSV path\n",
        "    text_column = 'english_text'       # or 'uzbek_text'\n",
        "\n",
        "    train_texts, train_labels = load_data_from_csv(train_csv_path, text_col=text_column)\n",
        "    val_texts, val_labels = load_data_from_csv(val_csv_path, text_col=text_column)\n",
        "\n",
        "    num_classes = len(set(train_labels))\n",
        "\n",
        "    tfidf_svm_train_predict(train_texts, train_labels, val_texts, val_labels)\n",
        "\n",
        "    fasttext_path = \"cc.ru.300.vec\"\n",
        "    if os.path.exists(fasttext_path):\n",
        "        ft_model, ft_dim = load_fasttext_embeddings(fasttext_path)\n",
        "        fasttext_train_predict(train_texts, train_labels, val_texts, val_labels, ft_model, ft_dim)\n",
        "    else:\n",
        "        print(f\"FastText embeddings file not found at {fasttext_path}. Skipping FastText model.\")\n",
        "\n",
        "    glove_path = \"glove.6B.100d.txt\"\n",
        "    if os.path.exists(glove_path):\n",
        "        train_bilstm(train_texts, train_labels, val_texts, val_labels, glove_path, num_classes)\n",
        "    else:\n",
        "        print(f\"GloVe embeddings file not found at {glove_path}. Skipping BiLSTM model.\")\n",
        "\n",
        "    model, tokenizer = mbert_train(train_texts, train_labels, val_texts, val_labels, num_classes)\n",
        "    mbert_predict(model, tokenizer, val_texts, val_labels)\n"
      ],
      "metadata": {
        "id": "34Ohjte5JJJB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}